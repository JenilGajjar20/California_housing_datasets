{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries\n",
    "\n",
    "- NumPy for numerical operations\n",
    "- scikit-learn for datasets and metrics \n",
    "- StandardScaler for feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "- Load the California housing dataset using **datasets.fetch_california_housing(return_X_y=True)**. The data is split into **features** (X) and **target values** (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.fetch_california_housing(return_X_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data\n",
    "\n",
    "- Create training and testing datasets: \n",
    "    - **X_train_temp1** contains the first 16,000 data points. \n",
    "    - **X_test_temp1** contains the remaining data points (from 16,000 to 20,603).\n",
    "- For both **X_train_temp1** and **X_test_temp1**, an extra column of ones is added at the beginning to represent the bias term.\n",
    "- The data is split into training and testing sets, and corresponding target values (**y_train** and **y_test**) are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train:  <class 'numpy.ndarray'> Shape of X_train:  (16000, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train_temp1 = X[0:16000, :]\n",
    "X_train = np.zeros((X_train_temp1.shape[0], X_train_temp1.shape[1] + 1))\n",
    "X_train[:, 0] = np.ones((X_train_temp1.shape[0]))\n",
    "X_train[:, 1:] = X_train_temp1\n",
    "print(\"Type of X_train: \", type(X_train), \"Shape of X_train: \", X_train.shape)\n",
    "\n",
    "y_train = y[0:16000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_test:  <class 'numpy.ndarray'> Shape of X_test:  (4604, 9)\n"
     ]
    }
   ],
   "source": [
    "X_test_temp1 = X[16000:20604]\n",
    "X_test = np.zeros((X_test_temp1.shape[0], X_test_temp1.shape[1] + 1))\n",
    "X_test[:, 0] = np.ones((X_test_temp1.shape[0]))\n",
    "X_test[:, 1:] = X_test_temp1\n",
    "print(\"Type of X_test: \", type(X_test), \"Shape of X_test: \", X_test.shape)\n",
    "\n",
    "y_test = y[16000:20604]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "- Create a **StandardScaler** object (**scaler**) and fit it on the training data's features (excluding the bias term, which is always 1).\n",
    "- Scale the features in both the training and testing sets using the fitted scaler. Feature scaling standardizes the data to have zero mean and unit variance, which can improve gradient descent convergence.\n",
    "\n",
    "\n",
    "### Model Initialization:\n",
    "\n",
    "- Initialize the model's parameters, represented by the vector **theta**, with random values between 0 and 1. The shape of **theta** is determined by the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of theta:  <class 'numpy.ndarray'> Shape of theta:  (9,)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[:, 1:])\n",
    "\n",
    "X_train[:, 1:] = scaler.transform(X_train[:, 1:])\n",
    "X_test[:, 1:] = scaler.transform(X_test[:, 1:])\n",
    "\n",
    "theta = np.random.uniform(0, 1, size = (X_train.shape[1]))\n",
    "print(\"Type of theta: \", type(theta), \"Shape of theta: \", theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model:\n",
    "\n",
    "- Set hyperparameters for training:\n",
    "    - **n_iterations** defines the number of gradient descent iterations.\n",
    "    - **alpha** is the learning rate.\n",
    "    - **lambda_reg** is the L2 regularization strength (ridge regression). It helps prevent overfitting by adding a penalty term to the cost function.\n",
    "    - **m** is the number of training examples.\n",
    "    - **n** is the number of features.\n",
    "\n",
    "\n",
    "### Loop over the specified number of iterations:\n",
    "\n",
    "- Initialize an array **update** to store the updates for each parameter.\n",
    "- Perform forward propagation to make predictions (**y_pred**) using the current parameters.\n",
    "- Calculate the error by subtracting the predicted values from the actual target values.\n",
    "- Loop over each feature to calculate the parameter updates.\n",
    "- Update each parameter **theta[j]** by applying gradient descent with L2 regularization.\n",
    "\n",
    "### Testing and Evaluation:\n",
    "\n",
    "- Use the trained model to make predictions on the testing set (**X_test**) and store the predictions in **predictions**.\n",
    "- Calculate and print the **Mean Absolute Error (MAE)** and **Mean Squared Error (MSE)** as evaluation metrics for the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of theta:  (9,)\n",
      "MAE:  0.5986529608385885\n",
      "MSE:  0.6732467179651381\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1000\n",
    "alpha = 0.01\n",
    "lambda_reg = 0.1\n",
    "m = X_train.shape[0]\n",
    "n = X_train.shape[1]\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    update = np.zeros(X_train.shape[1])\n",
    "    y_pred = np.dot(X_train, theta)\n",
    "    error = y_pred - y_train\n",
    "    for j in range(n):\n",
    "        update[j] = np.sum(error * (X_train.T)[j])\n",
    "    theta = (theta)*(1 - ((alpha)*(lambda_reg))/(m)) - (1/m)*(alpha)*(update)\n",
    "\n",
    "print(\"Shape of theta: \", theta.shape)\n",
    "\n",
    "predictions = np.dot(X_test, theta)\n",
    "\n",
    "print(\"MAE: \", metrics.mean_absolute_error(y_true = y_test, y_pred = predictions))\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_true = y_test, y_pred = predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
